diff -urNa src_old/devices/timer.c src/devices/timer.c
--- src_old/devices/timer.c	2020-09-09 19:14:35.122108000 +0800
+++ src/devices/timer.c	2021-11-29 13:36:53.997615000 +0800
@@ -89,11 +89,8 @@
 void
 timer_sleep (int64_t ticks) 
 {
-  int64_t start = timer_ticks ();
-
   ASSERT (intr_get_level () == INTR_ON);
-  while (timer_elapsed (start) < ticks) 
-    thread_yield ();
+  thread_sleep(ticks);
 }
 
 /* Sleeps for approximately MS milliseconds.  Interrupts must be
@@ -171,6 +168,15 @@
 timer_interrupt (struct intr_frame *args UNUSED)
 {
   ticks++;
+  handler();
+  increase_recentcpu();
+  if(ticks%TIMER_FREQ==0) {
+    update_loadavg();
+    update_recentcpu();
+  }
+  if(ticks%4==0) {
+    update_priority(thread_current());
+  }
   thread_tick ();
 }
 
diff -urNa src_old/threads/synch.c src/threads/synch.c
--- src_old/threads/synch.c	2020-09-09 19:14:35.134108000 +0800
+++ src/threads/synch.c	2021-11-29 13:48:15.315768000 +0800
@@ -113,10 +113,13 @@
   ASSERT (sema != NULL);
 
   old_level = intr_disable ();
-  if (!list_empty (&sema->waiters)) 
+  if (!list_empty (&sema->waiters))  {
+    list_sort (&sema->waiters, pri_cmp, NULL);
     thread_unblock (list_entry (list_pop_front (&sema->waiters),
                                 struct thread, elem));
+  }
   sema->value++;
+  thread_yield();
   intr_set_level (old_level);
 }
 
@@ -178,6 +181,7 @@
   ASSERT (lock != NULL);
 
   lock->holder = NULL;
+  lock->max_pri=-1;
   sema_init (&lock->semaphore, 1);
 }
 
@@ -196,8 +200,25 @@
   ASSERT (!intr_context ());
   ASSERT (!lock_held_by_current_thread (lock));
 
-  sema_down (&lock->semaphore);
-  lock->holder = thread_current ();
+  enum intr_level old_level;
+  old_level= intr_disable();
+  if(lock->holder!=NULL) {
+    thread_current()->blocking_lock=lock;//Set blocking_lock,if the thread is blocked,then it can be donated
+    struct lock* tmp_lock=lock;
+    while(tmp_lock && tmp_lock->holder->priority<thread_current()->priority) {
+      if(tmp_lock->holder->tmp_priority<0)
+        tmp_lock->holder->tmp_priority=tmp_lock->holder->priority;
+      tmp_lock->holder->priority=thread_current()->priority;
+      tmp_lock->max_pri=tmp_lock->max_pri>thread_current()->priority?tmp_lock->max_pri:thread_current()->priority;
+      tmp_lock=tmp_lock->holder->blocking_lock;
+    }
+    thread_yield();
+  } 
+  sema_down(&lock->semaphore);
+  thread_current()->blocking_lock=NULL;//Reset blocking_lock
+  list_insert_ordered(&thread_current()->locks, &lock->lockelem,lock_pri_cmp,NULL);
+  lock->holder = thread_current();
+  intr_set_level(old_level);
 }
 
 /* Tries to acquires LOCK and returns true if successful or false
@@ -228,11 +249,27 @@
 void
 lock_release (struct lock *lock) 
 {
+  int new_pri=thread_current()->tmp_priority;
   ASSERT (lock != NULL);
   ASSERT (lock_held_by_current_thread (lock));
-
+  enum intr_level old_level;
+  old_level= intr_disable();
   lock->holder = NULL;
+  list_remove(&lock->lockelem);
+  if(thread_current()->tmp_priority>=0) {
+    if(!list_empty(&thread_current()->locks)) {
+      int tp_pri=list_entry(list_front(&thread_current()->locks),struct lock,lockelem)->max_pri;
+      if(tp_pri>new_pri) {
+        new_pri=tp_pri;
+      }
+    }else {
+      thread_current()->tmp_priority=-1;
+    }
+    // thread_set_priority(new_pri);//wrong,order should not be changed
+    thread_current()->priority=new_pri;
+  }
   sema_up (&lock->semaphore);
+  intr_set_level(old_level);
 }
 
 /* Returns true if the current thread holds LOCK, false
@@ -296,6 +333,8 @@
   
   sema_init (&waiter.semaphore, 0);
   list_push_back (&cond->waiters, &waiter.elem);
+  // list_insert_ordered(&cond->waiters, &waiter.elem,con_pri_cmp,NULL);
+  //wrong,because hasn't been sema_down(),the con_pri_cmp can't find waiiters in semaphore
   lock_release (lock);
   sema_down (&waiter.semaphore);
   lock_acquire (lock);
@@ -316,9 +355,12 @@
   ASSERT (!intr_context ());
   ASSERT (lock_held_by_current_thread (lock));
 
-  if (!list_empty (&cond->waiters)) 
+  if (!list_empty (&cond->waiters)) {
+    list_sort (&cond->waiters, con_pri_cmp, NULL);
     sema_up (&list_entry (list_pop_front (&cond->waiters),
                           struct semaphore_elem, elem)->semaphore);
+  }
+  thread_yield();
 }
 
 /* Wakes up all threads, if any, waiting on COND (protected by
@@ -336,3 +378,12 @@
   while (!list_empty (&cond->waiters))
     cond_signal (cond, lock);
 }
+bool con_pri_cmp(const struct list_elem *a, const struct list_elem *b,void *aux) {
+  struct semaphore_elem *semaphore_a=list_entry(a,struct semaphore_elem,elem);
+  struct semaphore_elem *semaphore_b=list_entry(b,struct semaphore_elem,elem);
+  return list_entry(list_front(&(semaphore_a->semaphore.waiters)),struct thread,elem)->priority>
+  list_entry(list_front(&(semaphore_b->semaphore.waiters)),struct thread,elem)->priority;
+}
+bool lock_pri_cmp(const struct list_elem *a, const struct list_elem *b,void *aux) {
+  return list_entry(a,struct lock,lockelem)->max_pri>list_entry(b,struct lock,lockelem)->max_pri;
+}
diff -urNa src_old/threads/synch.h src/threads/synch.h
--- src_old/threads/synch.h	2020-09-09 19:14:35.134108000 +0800
+++ src/threads/synch.h	2021-11-29 13:45:01.310712000 +0800
@@ -22,6 +22,8 @@
   {
     struct thread *holder;      /* Thread holding lock (for debugging). */
     struct semaphore semaphore; /* Binary semaphore controlling access. */
+    struct list_elem lockelem;
+    int max_pri;
   };
 
 void lock_init (struct lock *);
@@ -41,6 +43,8 @@
 void cond_signal (struct condition *, struct lock *);
 void cond_broadcast (struct condition *, struct lock *);
 
+bool con_pri_cmp(const struct list_elem *a, const struct list_elem *b,void *aux);
+bool lock_pri_cmp(const struct list_elem *a, const struct list_elem *b,void *aux);
 /* Optimization barrier.
 
    The compiler will not reorder operations across an
diff -urNa src_old/threads/thread.c src/threads/thread.c
--- src_old/threads/thread.c	2020-09-09 19:14:35.134108000 +0800
+++ src/threads/thread.c	2021-11-29 13:49:54.067424000 +0800
@@ -23,7 +23,7 @@
 /* List of processes in THREAD_READY state, that is, processes
    that are ready to run but not actually running. */
 static struct list ready_list;
-
+static struct list sleeping_list;
 /* List of all processes.  Processes are added to this list
    when they are first scheduled and removed when they exit. */
 static struct list all_list;
@@ -58,7 +58,7 @@
    If true, use multi-level feedback queue scheduler.
    Controlled by kernel command-line option "-o mlfqs". */
 bool thread_mlfqs;
-
+static int64_t load_avg=0;
 static void kernel_thread (thread_func *, void *aux);
 
 static void idle (void *aux UNUSED);
@@ -92,7 +92,8 @@
   lock_init (&tid_lock);
   list_init (&ready_list);
   list_init (&all_list);
-
+  list_init(&sleeping_list);
+  
   /* Set up a thread structure for the running thread. */
   initial_thread = running_thread ();
   init_thread (initial_thread, "main", PRI_DEFAULT);
@@ -200,7 +201,9 @@
 
   /* Add to run queue. */
   thread_unblock (t);
-
+  if (thread_current()->priority<priority) {
+    thread_yield();
+  }
   return tid;
 }
 
@@ -237,7 +240,8 @@
 
   old_level = intr_disable ();
   ASSERT (t->status == THREAD_BLOCKED);
-  list_push_back (&ready_list, &t->elem);
+  list_insert_ordered(&ready_list,&t->elem,pri_cmp,NULL);
+  //list_push_back(&ready_list, &t->elem);
   t->status = THREAD_READY;
   intr_set_level (old_level);
 }
@@ -307,8 +311,10 @@
   ASSERT (!intr_context ());
 
   old_level = intr_disable ();
-  if (cur != idle_thread) 
-    list_push_back (&ready_list, &cur->elem);
+  if (cur != idle_thread) {
+    // list_push_back(&ready_list, &cur->elem);
+    list_insert_ordered(&ready_list,&cur->elem,pri_cmp,NULL);
+  }
   cur->status = THREAD_READY;
   schedule ();
   intr_set_level (old_level);
@@ -335,7 +341,15 @@
 void
 thread_set_priority (int new_priority) 
 {
-  thread_current ()->priority = new_priority;
+  enum intr_level old_level;
+  old_level = intr_disable();
+  struct thread* current=thread_current();
+  current->tmp_priority=new_priority;
+  if(list_empty(&current->locks)) {
+    current->priority = new_priority;
+    thread_yield();
+  } 
+  intr_set_level(old_level);
 }
 
 /* Returns the current thread's priority. */
@@ -349,31 +363,31 @@
 void
 thread_set_nice (int nice UNUSED) 
 {
-  /* Not yet implemented. */
+  thread_current()->nice=nice;
+  update_priority(thread_current());
+  if(thread_current()->priority!=PRI_MAX)
+    thread_yield();
 }
 
 /* Returns the current thread's nice value. */
 int
 thread_get_nice (void) 
 {
-  /* Not yet implemented. */
-  return 0;
+  return thread_current()->nice;
 }
 
 /* Returns 100 times the system load average. */
 int
 thread_get_load_avg (void) 
 {
-  /* Not yet implemented. */
-  return 0;
+  return round(load_avg*100);
 }
 
 /* Returns 100 times the current thread's recent_cpu value. */
 int
 thread_get_recent_cpu (void) 
 {
-  /* Not yet implemented. */
-  return 0;
+  return round(thread_current()->recent_cpu*100);
 }
 
 /* Idle thread.  Executes when no other thread is ready to run.
@@ -463,7 +477,11 @@
   t->stack = (uint8_t *) t + PGSIZE;
   t->priority = priority;
   t->magic = THREAD_MAGIC;
-
+  t->tmp_priority=-1;
+  t->blocking_lock=NULL;
+  t->nice=0;
+  list_init(&t->locks);
+  
   old_level = intr_disable ();
   list_push_back (&all_list, &t->allelem);
   intr_set_level (old_level);
@@ -582,3 +600,117 @@
 /* Offset of `stack' member within `struct thread'.
    Used by switch.S, which can't figure it out on its own. */
 uint32_t thread_stack_ofs = offsetof (struct thread, stack);
+
+void thread_sleep(int64_t ticks)
+{
+  struct thread *cur = thread_current();
+  enum intr_level old_level = intr_disable();
+  if (cur != idle_thread)
+  {
+    //list_push_back(&sleeping_list, &cur->slpelem);
+    cur->status = THREAD_SLEEPING;
+    cur->wake_time = timer_ticks() + ticks;
+    list_insert_ordered(&sleeping_list,&cur->slpelem,slp_cmp,NULL);
+    schedule();
+  }
+  intr_set_level(old_level);
+}
+
+void handler(void)
+{
+  struct list_elem *e = list_begin(&sleeping_list);
+  int64_t cur_ticks = timer_ticks();
+  enum intr_level old_level = intr_disable();
+  while (e != list_end(&sleeping_list))
+  {
+    struct thread *t = list_entry(e, struct thread, slpelem);
+    if (cur_ticks >= t->wake_time)
+    {
+      // list_push_back(&ready_list, &t->elem);
+      list_insert_ordered(&ready_list,&t->elem,pri_cmp,NULL);
+      t->status = THREAD_READY;
+      e = list_remove(e);
+    }
+    else
+      break;
+  }
+  intr_set_level(old_level);
+}
+
+bool pri_cmp(const struct list_elem *a,const struct list_elem *b,void *aux) {
+  return list_entry(a, struct thread, elem)->priority>list_entry(b, struct thread, elem)->priority;
+}
+
+bool slp_cmp(const struct list_elem *a,const struct list_elem *b,void *aux) {
+  return list_entry(a,struct thread, slpelem)->wake_time<list_entry(b,struct thread, slpelem)->wake_time;
+}
+void update_loadavg(void) {
+  //load_avg = (59/60)*load_avg + (1/60)*ready_threads.
+  if (thread_mlfqs) {
+    int ready_lists=list_size(&ready_list);
+    if(thread_current()!=idle_thread)
+      ready_lists++;
+    load_avg=load_avg/60*59+convert_to_fixed_point(ready_lists)/60;
+  }
+}
+
+void update_recentcpu(){
+  //recent_cpu = (2*load_avg)/(2*load_avg + 1) * recent_cpu + nice.
+  if (thread_mlfqs) {
+    struct list_elem *e = list_begin (&all_list);
+    for(;e!=list_end(&all_list);e=list_next(e)) {
+      struct thread* t=list_entry(e,struct thread,allelem);
+      if(t!=idle_thread) {
+        t->recent_cpu=fixed_fixed_multiply(fixed_fixed_divide(2*load_avg,2*load_avg+convert_to_fixed_point(1)),t->recent_cpu)+convert_to_fixed_point(t->nice);
+        update_priority(t);
+      }
+    }
+  }
+  
+}
+void update_priority(struct thread* t) {
+  //priority = PRI_MAX - (recent_cpu / 4) - (nice * 2).
+  if (thread_mlfqs) {
+    if(t!=idle_thread) {
+      t->priority=round(convert_to_fixed_point(PRI_MAX)-t->recent_cpu/4-convert_to_fixed_point(t->nice)*2);
+      if(t->priority>PRI_MAX)
+        t->priority=PRI_MAX;
+      else if(t->priority<PRI_MIN)
+        t->priority=PRI_MIN;
+    }
+  }
+}
+
+void increase_recentcpu() {
+  if(thread_mlfqs) {
+    if(thread_current()!=idle_thread)
+    thread_current()->recent_cpu=thread_current()->recent_cpu+convert_to_fixed_point(1);
+  }
+}
+
+//for fixed-point
+int64_t convert_to_fixed_point(int64_t a) {
+    return a * SHIFT;
+}
+
+int64_t round(int64_t a) {
+    if (a > 0) {
+        return (a + SHIFT / 2) / SHIFT;
+    } else if(a < 0) {
+        return (a - SHIFT / 2) / SHIFT;
+    } else {
+      return 0;
+    }
+}
+
+int64_t round_to_zero(int64_t a) {
+  return a / SHIFT;
+}
+
+int64_t fixed_fixed_multiply(int64_t a, int64_t b) {
+  return ((int64_t) a) * b / SHIFT;
+}
+
+int64_t fixed_fixed_divide(int64_t a,int64_t b) {
+  return ((int64_t) a) * SHIFT / b;
+}
diff -urNa src_old/threads/thread.h src/threads/thread.h
--- src_old/threads/thread.h	2020-09-09 19:14:35.134108000 +0800
+++ src/threads/thread.h	2021-11-29 13:38:55.502314000 +0800
@@ -11,6 +11,7 @@
     THREAD_RUNNING,     /* Running thread. */
     THREAD_READY,       /* Not running but ready to run. */
     THREAD_BLOCKED,     /* Waiting for an event to trigger. */
+    THREAD_SLEEPING,   /* New state for sleeping threads */
     THREAD_DYING        /* About to be destroyed. */
   };
 
@@ -88,10 +89,16 @@
     char name[16];                      /* Name (for debugging purposes). */
     uint8_t *stack;                     /* Saved stack pointer. */
     int priority;                       /* Priority. */
+    int tmp_priority;
+    int nice;
+    int64_t recent_cpu;
     struct list_elem allelem;           /* List element for all threads list. */
 
     /* Shared between thread.c and synch.c. */
     struct list_elem elem;              /* List element. */
+    struct list_elem slpelem;
+    struct list locks;      
+    struct lock* blocking_lock;
 
 #ifdef USERPROG
     /* Owned by userprog/process.c. */
@@ -100,6 +107,7 @@
 
     /* Owned by thread.c. */
     unsigned magic;                     /* Detects stack overflow. */
+    int64_t wake_time;
   };
 
 /* If false (default), use round-robin scheduler.
@@ -138,4 +146,21 @@
 int thread_get_recent_cpu (void);
 int thread_get_load_avg (void);
 
+void handler(void);
+void thread_sleep(int64_t);
+bool pri_cmp(const struct list_elem *,const struct list_elem *,void *aux);
+bool slp_cmp(const struct list_elem *,const struct list_elem *,void *aux);
+void update_loadavg(void);
+void update_recentcpu();
+void increase_recentcpu();
+void update_priority(struct thread*);
+
+//for fixed-point
+#define SHIFT (1<<18)
+int64_t convert_to_fixed_point(int64_t);
+int64_t round(int64_t);
+int64_t round_to_zero(int64_t);
+int64_t fixed_fixed_multiply(int64_t,int64_t);
+int64_t fixed_fixed_divide(int64_t,int64_t);
+
 #endif /* threads/thread.h */
